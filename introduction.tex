\vspace{-0.75cm}

\section{Introduction} \label{sec:introduction}
    \IEEEPARstart{U}{nder} the umbrella of \glss{ILD}, \gls{IPF} is  characterised by a buildup of scar tissue in and a stiffening of the lungs of the patient; leading to a reduction in the volume of the lung, resulting in a shortness of breath and eventually death. As with many other \glss{ILD}, the progression of the disease is heterogeneous, and prognosis is challenging~\cite{King2011IdiopathicFibrosis}.
    
    Previously, methods to monitor \gls{IPF} included testing lung function, with spirometric measurement of lung volume~\cite{Watters1986AFibrosis}, or taking \gls{CT} acquisitions over time~\cite{Lynch2018DiagnosticPaper}. Both approaches are limited by several factors, including, physical limitations of the patient, technical accuracy (spirometer baseline drift bias), and  longitudinal data availability. In contrast, measuring how long an \gls{IPF} patient could survive on the basis of a baseline \gls{CT} scan can be a useful clinical outcome to measure in order to prioritise resources and plan intervention. %Furthermore, for research, it would be useful to accurately predict survival time in order to perform analysis of the disease itself.
    For this reason, here the focus is on accurately predicting \gls{IPF} patient death time based on a baseline \gls{CT} scan and associated clinical features.

    In Cox Proportional Hazards Survival Analysis~\cite{Cox1972RegressionLife-Tables} the death time of one participant is compared to another, and the model ranks patients according to their expected death times. However, a limitation is that the model does not directly output survival times. Recently models have been introduced that attempt to predict more directly the death time of a participant. For instance, DeepHit uses a \gls{CNN} to perform feature extraction on input \gls{CT} and outputs the probability of a survival time falling within predetermined bins of a death time histogram~\cite{Lee2018DeepHit:Risks}. This treats survival analysis as a multi-class classification problem with a class for each time-bin that a patient could die in. A disadvantage is that the bins are not ordinally related and the model is penalised as much for making an error of one month as it is an error of ten years.

    Censoring is a significant issue in survival analysis in which the precise death time of a patient is unknown. %For example, it may be known only that the patient is still alive, or that the patient died only within a specific time interval. 
    In the \gls{OSIC} \gls{IPF} data set~\cite{OSICOSICRepository}, approximately \SI{66}{\percent} of the records are right-censored, meaning that the time of death is above a known value but it is unknown by how much. A simple approach would be to remove censored data, however, this would discard a very significant fraction of training data. Missing data in clinical records is a related issue. Again, with approximately \SI{66}{\percent} of the \gls{OSIC} \gls{IPF} data~\cite{OSICOSICRepository} set has some missing clinical information.

    Here, a number of survival analysis models that predict death time using a \gls{NN} are presented. The inputs of which being a baseline \gls{CT} scan and associated patient clinical information (such as height, age, etc). The models are able to address censoring and missing clinical information following~\cite{Shahin2023DeepAnalysis, Shahin2022SurvivalData} respectively. Different training losses are used, including ones based on classical Cox based ranking, likelihood, and DeepHit. In the case of the Cox based loss, one with and one without a memory bank of previous predictions is used (to allow the loss to be approximated at all previously seen data points~\cite{Shahin2022SurvivalData}). In the case of the likelihood based loss, one where censoring time is sampled in the classical way and one where censoring time is sampled from a uniform distribution is used~\cite{Shahin2023DeepAnalysis}. Extensions over previous work include; a change to the \gls{NN} architecture (larger, with a learnt downsampling, parameterised activation and softplus output, and orthogonal initialisation), a new optimiser, gradient accumulation (as such an increased batch size), and an annealed learning rate.
