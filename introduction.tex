% \vspace{-0.5cm}

\section{Introduction} \label{sec:introduction}
    \IEEEPARstart{U}{nder} the umbrella of \gls{ILD}, \gls{IPF} is  characterised by a buildup of scar tissue in and a stiffening of the lungs of the patient; leading to a reduction in the volume of the lung, resulting in a shortness of breath. As with many other \glss{ILD}, the progression of the disease is heterogeneous, and prognosis is challenging~\cite{King2011IdiopathicFibrosis}. 
    
    Previously, methods to monitor \gls{IPF} included testing lung function, with spirometric measurement of lung volume~\cite{Watters1986AFibrosis}, or taking \gls{CT} acquisitions over time~\cite{Lynch2018DiagnosticPaper}. Both approaches are limited by several factors, including, physical limitations of the patient, technical accuracy (spirometer baseline drift bias), and  longitudinal data availability \gls{CT}.  In contrast, measuring how long a \gls{IPF} patient will survive on the basis of a baseline \gls{CT} scan can be a useful clinical outcome to measure in order to prioritise resources and plan intervention. Furthermore, for research, it would be useful to accurately predict survival time in order to perform analysis of the disease itself. For this reason, our focus is on accurately predicting \gls{IPF} patient death time based on a baseline \gls{CT} scan and associated clinical data.

    In Cox Proportional Hazards Survival Analysis~\cite{Cox1972RegressionLife-Tables} the death time of one participant is compared to another, and the model ranks patients according to their expected death times. However, a limitation is that the model does not directly output survival times.  More recently models have been introduced that attempt to predict more directly the death time of a participant. For instance, DeepHit uses a \gls{CNN} to perform feature extraction on input \gls{CT} and outputs the probability of a survival time falling within predetermined bins of a death time histogram~\cite{Lee2018DeepHit:Risks}. This treats survival analysis as a multi-class classification problem with a class for each time-bin that a patient could die in. A disadvantage is that the bins are not ordinally related and the model is penalised as much for making an error of 1 month as it is an error of 10 years.

    Censoring is a significant issue in survival in which the precise death time of a patient is unknown. For example, it may be known only that the patient is still alive, or that the patient died only within a specific time interval. In our \gls{IPF} data, x\% of the records are right-censored (meaning what?). A simple approach would be to simply remove censored data -- however, that would remove a very significant fraction of training data, reducing the effectiveness of parameter fitting.% If all censored data were removed from a study then often it would dramatically reduce the available data.
    
    Missing data in clinical records is a related significant issue. y\% of our data points have some missing clinical information. 

    We present a survival analysis model that predicts death time using a deep neural network, with inputs being a baseline \gls{CT} scan and associated patient clinical information (such as height, age, etc). The model is able to deal with censoring and missing clinical information (David --- you don't give much information about how to do this in the methods section).
    
    Different training loss functions are used, including a classical Cox based ranking one and a likelihood based one (David -- I thought there were two likelihood based losses?). In the case of the Cox based loss, a memory bank of previous predictions is used to allow the loss to be approximated at all previously seen data points (David -- I'm not sure anyone can understand this without reference to the previous work). In the case of the likelihood based loss, censoring time is sampled from a uniform distribution. Extensions over previous work \cite{} include a change to the \gls{NN} architecture (larger, with a learnt downsampling, parameterised activation and softplus output, and orthogonal initialisation), a new optimiser, gradient accumulation (as such an increased batch size), and an annealed learning rate.
