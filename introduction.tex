% \vspace{-0.5cm}

\section{Introduction} \label{sec:introduction}
    \IEEEPARstart{T}{here} are a myriad of diseases under the umbrella of \gls{ILD}. \gls{IPF} is one such disease characterised by a buildup of scar tissue in and a stiffening of the lungs of the patient. This scarring and stiffening of the lungs leads to a reduction in the volume of the lung and a shortness of breath. As with many other \gls{ILD} the progression of the disease is heterogeneous, meaning that from any given state it is not easily possible to predict any future states nor an expected survival time for the patient~\cite{King2011IdiopathicFibrosis}. It would be useful, clinically, to be able to predict an expected survival time both due to prioritising resources as well as planning intervention. For research, it would be useful to be able to accurately predict survival time in order to perform analysis of the disease itself.

    Previously, methods to monitor \gls{IPF} included such methods as to test lung function (spirometric measurement of lung volume) or take \gls{CT} acquisitions over time~\cite{Watters1986AFibrosis, Lynch2018DiagnosticPaper}. Issues with these methods include; the introduction of bias, because of complications such as baseline drift in spirometer measurements, as well as an increased dose to the patient due to the necessity of multiple sequential \gls{CT} acquisitions. Furthermore, each of the above methods requires that some time passes between data points, which is not ideal. Regardless, it's uncertain how much information about future progression can be ascertained directly through these observations because of the non-linearity of the disease.

    Other methods to predict survival time include survival analysis. This is a group of methods which can be used to predict the proportion of a population that will 'live' past a certain point. Survival analysis has not only been used for medical research but is also useful in the prediction of failure times of mechanical systems. One example model is the Cox Proportional Hazards model~\cite{Cox1972RegressionLife-Tables}. Here, the failure point of one participant is compared to another. However, a limitation is that the model does not directly output survival times.

    More recently \gls{NN} based methods have become more popular to tackle similar problems. For instance, DeepHit uses a \gls{CNN} to perform feature extraction on input \gls{CT} data before using a fully connected \gls{NN} to output the probability of a survival time falling within predetermined bins of a histogram~\cite{Lee2018DeepHit:Risks}. A disadvantage of this method is that the output of the method is discretised rather than continuous, which not only limits the resolution of the output but also means that it is a lot more difficult to both use the output in certain circumstances as well as apply regularisation during training.

    An issue prevalent in all survival analysis methods is that of censoring. This is when a data point exists for a subject but it survived past the end of a relevant study. Thus the survival time of the subject is unknown other than it is greater than a fixed point. If all censored data were removed from a study then often it would dramatically reduce the available data.

    Here, an extension of previous work is presented which makes use of a combined \gls{CNN} feature extractor and fully connected \gls{NN} to perform survival analysis. Different loss functions are used, including; a Cox based and a likelihood based one. In the case of the Cox based loss, a memory bank of previous predictions is used to allow the loss to be approximated at all previously seen data points. In the case of the likelihood based loss, censoring time is sampled from a uniform distribution. Extensions over previous work include; a change to the \gls{NN} architecture (larger, with a learnt downsampling, parameterised activation and softplus output, and orthogonal initialisation), a new optimiser, gradient accumulation (as such an increased batch size), and an annealed learning rate.
